# LR

#### LR推导

**https://www.cnblogs.com/bonelee/p/7253508.html**



#### LR损失函数是什么？优缺点，适用场景

>**损失函数**：极小化对数似然的相反数：
>
>![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Chat%7By%7D%2Cy%29+%3D+-log%28%5Chat%7By%7D%5E%7By%7D%281-%5Chat%7By%7D%29%5E%7B%281-y%29%7D%29%3D-%5Bylog%5Chat%7By%7D%2B%281-y%29log%281-%5Chat%7By%7D%29%5D)
>
>优势：
>
>-  模型简单
>- 可解释性强
>
>缺点：
>
>- 处理不好特征间的关系
>- 特征空间很大时，性能不好
>- 容易欠拟合，精度不高
>
>适用情况：
>
>​	样本线性可分且样本空间不是很大的情况



#### LR的逻辑函数为什么使用sigmod函数，这个函数有什么优点和缺点？

> 首先LR本身就是线性回归针对Sigmod的应用，之所以要应用Sigmod在线性回归之上是因为线性回归是一种回归模型，而Sigmod具有一个很良好的性质可以将任何数值转化成一个0~1之间的数，因此可采用sigmod函数来将回归问题转化为分类问题。
>
> Sigmod函数
>
> 优点：1.实现简单，可无限求导 2.可以将任意数值转化成0~1的值
>
> 缺点：1.容易出现梯度消失 2.只能处理二分类问题







#### 逻辑回归与线性回归有什么异同点？

> 相同点：
>
> - 都使用极大似然估计来对训练样本进行建模
> - 都使用梯度下降算法来求解超参数
>
> 不同点：
>
> - 本质区别：逻辑回归处理分类问题，线性回归处理回归问题

#### LR和SVM的区别

> 1. LR是基于概率推导的，SVM是基于最大化几何间隔
>
> 2. SVM的决策面只由少数支持向量决定，LR的决策面则是由全部训练集样本决定的
>
> 3. SVM对于异常点不敏感，LR对于异常点敏感
>
> 4. SVM依赖于距离度量，因此必须要在进入模型前先将数据进行normalization，LR则可以不做

#### 在SVM常常引入核函数，LR能不能使用？为什么？

> 在原理上LR是可以使用核函数的，但是之所以很听到使用核函数的SVM是因为LR是每个样本都要参与决策，因此，每个样本的值都需要进行核变化，计算量非常大，因此很少使用。而SVM只有处于最大间隔超平面上的样本点才参与决策，因此计算量较少，因此使用核函数较多。

#### 
