# XGBoost

#### XGBoost的损失函数是什么，节点划分准则是什么？

> 损失函数：
>
> ![img](https://github.com/AnchoretY/images/blob/master/blog/xgboost%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png?raw=True)
>
>  节点划分准则：
>
> - 分类树：信息增益获信息增益比
>
> - 回归树：最大均方误差



**整体流程：**

**Xgboost和GBDT算法时间复杂度是多少？**

 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）

 时间复杂度:**O(nlogn \*d\* m)**(n是样本个数，d是特征个数,m是树的深度)

**xgboost是如何进行剪枝的？**

 xgboost采用**后剪枝**的方式进行剪枝，即 从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝，这样不容易陷入局部最优解。

**xgboost和gbdt的区别：**

 1.xgboost是gbdt的工程化实现

 2.xgboost加入了正则化信息

 3.xgboost允许使用自定义的损失函数

 4.xgboost损失函数加入了二阶导数信息，下降的更快更准

 5.xgboost支持和随机森林一样的列抽样

 6.xgboost支持并行化，但是并不是树与树之间的并行化，而是在最费时的特征排序截断进行并行化，将排序结果进行分桶保存，各个树生成时复用

 7.xgboost基模型除了支持gbdt支持的CART树外还支持其他的基模型
